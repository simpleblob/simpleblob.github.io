#+TITLE: PatrickSu deep learning paper reading notes
#+DESCRIPTION: based on awesome list papers
#+LINK: https://github.com/terryum/awesome-deep-learning-papers
#+KEYWORDS: CNN

---
title: deep learning paper reading notes
subtitle: none
description: dl paper notes
tags: deep_learning
created: 2018-01-11
published: 2018-01-11
status: ongoing
confidence: log
importance: 1
---

* Background
Before this list, there exist other awesome deep We believe that there exist 
/classic/ deep learning papers which are worth reading regardless of their application domain. 
Rather than providing overwhelming amount of papers, We would like to provide a
/curated list/ of the awesome deep learning papers which are considered
as /must-reads/ in certain research domains.
learning lists/, for example, [[https://github.com/kjw0612/awesome-deep-vision][Deep Vision]] and [[https://github.com/kjw0612/awesome-rnn][Awesome Recurrent Neural Networks]]. 
Also, after this list comes out, another awesome list for
deep learning beginners, called [[https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap][Deep Learning Papers Reading Roadmap]], 
has been created and loved by many deep learning researchers.

* Famous Machine Learning Conferences
** NIPS
 :TOPIC: general machine learning
 :URL: https://nips.cc/
** CVPR
 :TOPIC: computer vision (US)
 :URL: http://cvpr2019.thecvf.com/
** ECCV
 :TOPIC: computer vision (european)
 :URL: https://eccv2018.org/
** ICML
 :TOPIC: general machine learning (international)
 :URL: https://icml.cc/
** ICCV
 :TOPIC: computer vision (international)
 :URL: http://iccv2019.thecvf.com/submission/timeline
** SIGGRAPH
 :TOPIC: animation, computer graphic
 :URL: https://www.siggraph.org/
* Famous Challenges / Dataset
** list: https://competitions.codalab.org/
** [2010-2017] ImageNet
 :SIZE: 14 mil with 20k synset for classification / 1 mil with bboxs and 200 class
 :TOPIC: Image classification and object detection
 :URL: http://image-net.org/about-stats
** [2005-2012] Pascal VOC
 :SIZE: (2007) 10k imgs with 20 class / (2012) 12k images with 20 class & 7k imgs for segmentation
 :TOPIC: Image classification, detection, segmentation
 :URL: http://image-net.org/about-stats
* Understanding / Generalization / Transfer
** [2015] Distilling the knowledge in a neural network
   :PROPERTIES:
   :AUTHOR:   G. Hinton et al.
   :YEAR:     2015
   :URL:      http://arxiv.org/pdf/1503.02531
   :END:
*** keypoints
**** train the complex model first (model-A) 
**** then train a simpler one using loss function that combines (same dataset) and (model-A prediction)
** [2015] Deep neural networks are easily fooled: High confidence predictions for unrecognizable images 
   :PROPERTIES:
   :AUTHOR:   A. Nguyen et al.
   :YEAR:     2015
   :URL:      http://arxiv.org/pdf/1412.1897
   :END:
*** keypoints
**** use the CNN model's prediction probabilities as input
**** use an evolution algorithm to evolve a random image to fool the model
**** some images are similar to the "real" thing, some looks just like static TV noise
**** using the "static" images to retrain, still difficult to patch up the weakness
**** is this similar to adversarial network?
** [2014] How transferable are features in deep neural networks?
   :PROPERTIES:
   :AUTHOR:   J. Yosinski et al.
   :YEAR:     2014
   :URL:      http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf
   :END:
*** keypoints
**** through empirical evidence, researchers notice that for all CNN models, the first 1-3 layers are similar
**** the higher layers (after three) are more specific to the classification task
**** we want to test how "general" or "specific" for each layer
**** train a real-image classification CNN (7 layers) model-A and model-B, using completely seperate classes
**** freeze 3 lowest layers from model A, then put the 4 higher layer with random weight, then train with model B dataset
**** the resulting accuracy does not change 
**** and actually if we don't freeze (let it fine-tune), the accuracy is higher (it generalizes better)
** [2014] CNN features off-the-Shelf: An astounding baseline for recognition
   :PROPERTIES:
   :AUTHOR:   Razavian et al.
   :YEAR:     2014
   :URL:      http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf
   :END:
*** keypoints
**** comparison of state-of-the-art "manual" feature engineering (SIFT etc.) vs "OVERFEAT" CNN
**** Summmary from the paper: 
It’s all about the features! SIFT and HOG descriptors produced big performance gains a decade ago and
now deep convolutional features are providing a similar breakthroughfor recognition. 
Thus, applying the well-established com-puter vision procedures on CNN representations should 
potentially push the reported results even further. In any case,if you develop any new algorithm for 
a recognition task thenitmustbe compared against the strong baseline ofgenericdeep features+simple classifier.
** [2014] Learning and transferring mid-Level image representations using convolutional neural networks 
   :PROPERTIES:
   :AUTHOR:   M. Oquab et al.
   :YEAR:     2014
   :URL:      http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf
   :END:
*** keypoints
**** same idea as the "transferable features in DNN" paper
**** use the pre-trained weights from task A (ImageNet) to apply to task B (Pascal)
**** they transferred all the weights (all CNN and FCs layers), froze them , and added 2 FC layers at the end to adapt to new output
**** for task B (Pascal), the pictures are cropped to specific object, so they use a sliding window to generate new pics + "background" class
** [2014] Visualizing and understanding convolutional networks
   :PROPERTIES:
   :AUTHOR:   M. Zeiler and R. Fergus
   :YEAR:     2014
   :URL:      http://arxiv.org/pdf/1311.2901
   :END:
*** keypoints
**** Building from 2011 papers, they use deconvnet to analyze the CNN layers.
** Decaf: A deep convolutional activation feature for generic visual recognition* (2014), J. Donahue et al.
  [[http://arxiv.org/pdf/1310.1531][[pdf]]]

* Optimization / Training Techniques
** *Batch normalization: Accelerating deep network training by reducing internal covariate shift* 
*** (2015), S. Loffe and C. Szegedy
*** [[http://arxiv.org/pdf/1502.03167][[pdf]]]
** *Delving deep into rectifiers: Surpassing human-level performance on imagenet classification* (2015), K. He et al.
  [[http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf][[pdf]]]
** *Dropout: A simple way to prevent neural networks from overfitting* (2014), N. Srivastava et al.
  [[http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf][[pdf]]]
** *Adam: A method for stochastic optimization* (2014), D. Kingma and J.
  Ba [[http://arxiv.org/pdf/1412.6980][[pdf]]]
** *Improving neural networks by preventing co-adaptation of feature
  detectors* (2012), G. Hinton et al.
  [[http://arxiv.org/pdf/1207.0580.pdf][[pdf]]]
** [2012] Random search for hyper-parameter optimization
   :PROPERTIES:
   :AUTHOR:   M. Zeiler and R. Fergus
   :YEAR:     2012
   :URL:      http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a
   :END:
** [2017] A summary of gradient descent optimization algorithms
   :PROPERTIES:
   :AUTHOR:   M. Zeiler and R. Fergus
   :YEAR:     2014
   :URL:      http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms
   :END:
*** keypoints
**** *TLDR; === Use Adam, then try others if it doesn't work ===
**** SGD - basic gradient descent
**** mini-batch - update once every batch
**** online - update once every sample
**** momentum - running faster and faster into the general direction of local minima
**** Nesterov - to prevent overshooting cause by momentum, we can "correct" it by first calculate momentum, then add the loss of current param diff with the momentum.
**** Adagrad - it has a unique learning rate for each parameter i. The learning rate is normalized based on past gradient values of that parameters. Weakness is that it makes learning rates go infinitely small.
**** Adadelta - fix the learning rate shrinking problem. by replacing the scaling term with RMSE.
**** RMSprop - similar to Adadelta, developed by Hinton during class.
**** Adam - has first and second moments of gradients. essentially Momentum + RMSprop
**** AdaMax - generalized Adam to n moments
**** Nadam - Nesterov + Adam 
* Unsupervised / Generative Models
** *Pixel recurrent neural networks* (2016), A. Oord et al.
  [[http://arxiv.org/pdf/1601.06759v2.pdf][[pdf]]]
** *Improved techniques for training GANs* (2016), T. Salimans et al.
  [[http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf][[pdf]]]
** *Unsupervised representation learning with deep convolutional
  generative adversarial networks* (2015), A. Radford et al.
  [[https://arxiv.org/pdf/1511.06434v2][[pdf]]]
** *DRAW: A recurrent neural network for image generation* (2015), K.
  Gregor et al. [[http://arxiv.org/pdf/1502.04623][[pdf]]]
** *Generative adversarial nets* (2014), I. Goodfellow et al.
  [[http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf][[pdf]]]
** *Auto-encoding variational Bayes* (2013), D. Kingma and M. Welling
  [[http://arxiv.org/pdf/1312.6114][[pdf]]]
** *Building high-level features using large scale unsupervised
   learning* (2013), Q. Le et al.
   [[http://arxiv.org/pdf/1112.6209][[pdf]]]

#+BEGIN_HTML
  <!---[Key researchers] [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Ian Goodfellow](https://scholar.google.ca/citations?user=iYN86KEAAAAJ), [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)-->
#+END_HTML

* CNN Feature Extractors
** *Rethinking the inception architecture for computer vision* (2016),
  C. Szegedy et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf][[pdf]]]
** *Inception-v4, inception-resnet and the impact of residual
  connections on learning* (2016), C. Szegedy et al.
  [[http://arxiv.org/pdf/1602.07261][[pdf]]]
** *Identity Mappings in Deep Residual Networks* (2016), K. He et al.
  [[https://arxiv.org/pdf/1603.05027v2.pdf][[pdf]]]
** *Deep residual learning for image recognition* (2016), K. He et al.
  [[http://arxiv.org/pdf/1512.03385][[pdf]]]
** *Spatial transformer network* (2015), M. Jaderberg et al.,
  [[http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf][[pdf]]]
** *Going deeper with convolutions* (2015), C. Szegedy et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf][[pdf]]]
** *Very deep convolutional networks for large-scale image recognition*
  (2014), K. Simonyan and A. Zisserman
  [[http://arxiv.org/pdf/1409.1556][[pdf]]]
** *Spatial pyramid pooling in deep convolutional networks for visual
  recognition* (2014), K. He et al.
  [[http://arxiv.org/pdf/1406.4729][[pdf]]]
** *Return of the devil in the details: delving deep into convolutional
  nets* (2014), K. Chatfield et al.
  [[http://arxiv.org/pdf/1405.3531][[pdf]]]
** *OverFeat: Integrated recognition, localization and detection using
  convolutional networks* (2013), P. Sermanet et al.
  [[http://arxiv.org/pdf/1312.6229][[pdf]]]
** *Maxout networks* (2013), I. Goodfellow et al.
  [[http://arxiv.org/pdf/1302.4389v4][[pdf]]]
** *Network in network* (2013), M. Lin et al.
  [[http://arxiv.org/pdf/1312.4400][[pdf]]]
** *ImageNet classification with deep convolutional neural networks*
   (2012), A. Krizhevsky et al.
   [[http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf][[pdf]]]

#+BEGIN_HTML
  <!---[Key researchers]  [Christian Szegedy](https://scholar.google.ca/citations?hl=en&user=3QeF7mAAAAAJ), 
[Kaming He](https://scholar.google.ca/citations?hl=en&user=DhtAFkwAAAAJ), 
[Shaoqing Ren](https://scholar.google.ca/citations?hl=en&user=AUhj438AAAAJ), [Jian Sun](https://scholar.google.ca/citations?hl=en&user=ALVSZAYAAAAJ), 
[Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Yann LeCun](https://scholar.google.ca/citations?hl=en&user=WLN3QrAAAAAJ)-->
#+END_HTML
* Image: Object Detection
** Overview paper: [2018-09] recent advances in object detection in the age of deep CNNs
*** https://arxiv.org/pdf/1809.03193.pdf
** YOLO family
*** YOLOv1
**** simple network design, one-shot detector
**** result (voc 07-12) - mAP(0.5) 63.4 with 45 FPS at 554x554 on Titan X
*** YOLOv2
**** add batch normalization, able to train deeper network
**** double input resolution 224x224 --> 448x448 (also in Imagenet pretraining)
**** add anchor box priors, will custom clustering to find best priors
**** result (voc 07-12) - mAP(0.5) 78.6 with 40 FPS at 554x554 on Titan X
*** YOLOv3
**** predict boxes at 3 different scales (similar to SSD)
**** use skip connection (upsampled then concat layers)
**** much deeper feature extractors (Darknet-53)
**** result (COCO) - mAP(0.5) 57.9 with 20 FPS at 608x608 on Titan X
** R-CNN family
*** source: http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf
*** History
**** R-CNN: Selective search → Cropped Image → CNN  
**** Fast R-CNN: Selective search → Crop feature map of CNN
**** Faster R-CNN: CNN → Region-Proposal Network → Crop feature map of CN** 
*** Best accuracy but slow
* Image: Segmentation
** [2015] Fully convolutional networks for semantic segmentation 
   :PROPERTIES:
   :AUTHOR:   J. Long et al.
   :YEAR:     2015
   :URL:      http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf
   :END:
*** keypoints
**** demonstrate an fully CNN without FC layers at the end -- without additional manual manipulation
** *Rich feature hierarchies for accurate object detection and semantic segmentation* (2014), R. Girshick et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf][[pdf]]]
** *Semantic image segmentation with deep convolutional nets and fully connected CRFs*, L. Chen et al.
  [[https://arxiv.org/pdf/1412.7062][[pdf]]]
** *Learning hierarchical features for scene labeling* (2013), C. Farabet et al.
  [[https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf][[pdf]]]

* Image / Video / Etc
** *Image Super-Resolution Using Deep Convolutional Networks* (2016), C.
  Dong et al. [[https://arxiv.org/pdf/1501.00092v3.pdf][[pdf]]]
** *A neural algorithm of artistic style* (2015), L. Gatys et al.
  [[https://arxiv.org/pdf/1508.06576][[pdf]]]
** *Deep visual-semantic alignments for generating image descriptions*
  (2015), A. Karpathy and L. Fei-Fei
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf][[pdf]]]
** *Show, attend and tell: Neural image caption generation with visual
  attention* (2015), K. Xu et al.
  [[http://arxiv.org/pdf/1502.03044][[pdf]]]
** *Show and tell: A neural image caption generator (2015)*, O. Vinyals
  et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf][[pdf]]]
** *Long-term recurrent convolutional networks for visual recognition
  and description* (2015), J. Donahue et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf][[pdf]]]
** *VQA: Visual question answering* (2015), S. Antol et al.
  [[http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf][[pdf]]]
** *DeepFace: Closing the gap to human-level performance in face
  verification* (2014), Y. Taigman et al.
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf][[pdf]]]:
** *Large-scale video classification with convolutional neural networks*
  (2014), A. Karpathy et al.
  [[http://vision.stanford.edu/pdf/karpathy14.pdf][[pdf]]]
** *DeepPose: Human pose estimation via deep neural networks* (2014), A.
  Toshev and C. Szegedy
  [[http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf][[pdf]]]
** *Two-stream convolutional networks for action recognition in videos*
  (2014), K. Simonyan et al.
  [[http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf][[pdf]]]
** *3D convolutional neural networks for human action recognition*
   (2013), S. Ji et al.
   [[http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf][[pdf]]]

#+BEGIN_HTML
  <!---[Key researchers]  [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Andrej Karpathy](https://scholar.google.ca/citations?user=l8WuQJgAAAAJ)-->
#+END_HTML

#+BEGIN_HTML
  <!---[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)-->
#+END_HTML

* Natural Language Processing / RNNs
** *Neural Architectures for Named Entity Recognition* (2016), G. Lample
  et al. [[http://aclweb.org/anthology/N/N16/N16-1030.pdf][[pdf]]]
** *Exploring the limits of language modeling* (2016), R. Jozefowicz et
  al. [[http://arxiv.org/pdf/1602.02410][[pdf]]]
** *Teaching machines to read and comprehend* (2015), K. Hermann et al.
  [[http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf][[pdf]]]
** *Effective approaches to attention-based neural machine translation*
  (2015), M. Luong et al. [[https://arxiv.org/pdf/1508.04025][[pdf]]]
** *Conditional random fields as recurrent neural networks* (2015), S.
  Zheng and S. Jayasumana.
  [[http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf][[pdf]]]
** *Memory networks* (2014), J. Weston et al.
  [[https://arxiv.org/pdf/1410.3916][[pdf]]]
** *Neural turing machines* (2014), A. Graves et al.
  [[https://arxiv.org/pdf/1410.5401][[pdf]]]
** *Neural machine translation by jointly learning to align and
  translate* (2014), D. Bahdanau et al.
  [[http://arxiv.org/pdf/1409.0473][[pdf]]]
** *Sequence to sequence learning with neural networks* (2014), I.
  Sutskever et al.
  [[http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf][[pdf]]]
** *Learning phrase representations using RNN encoder-decoder for
  statistical machine translation* (2014), K. Cho et al.
  [[http://arxiv.org/pdf/1406.1078][[pdf]]]
** *A convolutional neural network for modeling sentences* (2014), N.
  Kalchbrenner et al. [[http://arxiv.org/pdf/1404.2188v1][[pdf]]]
** *Convolutional neural networks for sentence classification* (2014),
  Y. Kim [[http://arxiv.org/pdf/1408.5882][[pdf]]]
** *Glove: Global vectors for word representation* (2014), J. Pennington
  et al. [[http://anthology.aclweb.org/D/D14/D14-1162.pdf][[pdf]]]
** *Distributed representations of sentences and documents* (2014), Q.
  Le and T. Mikolov [[http://arxiv.org/pdf/1405.4053][[pdf]]]
** *Distributed representations of words and phrases and their
  compositionality* (2013), T. Mikolov et al.
  [[http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf][[pdf]]]
** *Efficient estimation of word representations in vector space*
  (2013), T. Mikolov et al. [[http://arxiv.org/pdf/1301.3781][[pdf]]]
** *Recursive deep models for semantic compositionality over a sentiment
  treebank* (2013), R. Socher et al.
  [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&rep=rep1&type=pdf][[pdf]]]
** *Generating sequences with recurrent neural networks* (2013), A.
   Graves. [[https://arxiv.org/pdf/1308.0850][[pdf]]]

#+BEGIN_HTML
  <!---[Key researchers]  [Kyunghyun Cho](https://scholar.google.ca/citations?user=0RAmmIAAAAAJ), [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Richard Socher](https://scholar.google.ca/citations?hl=en&user=FaOcyfMAAAAJ), [Tomas Mikolov](https://scholar.google.ca/citations?user=oBu8kMMAAAAJ), [Christopher D. Manning](https://scholar.google.ca/citations?user=1zmDOdwAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ)-->
#+END_HTML

* Speech / Other Domain
** *End-to-end attention-based large vocabulary speech recognition*
  (2016), D. Bahdanau et al.
  [[https://arxiv.org/pdf/1508.04395][[pdf]]]
** *Deep speech 2: End-to-end speech recognition in English and
  Mandarin* (2015), D. Amodei et al.
  [[https://arxiv.org/pdf/1512.02595][[pdf]]]
** *Speech recognition with deep recurrent neural networks* (2013), A.
  Graves [[http://arxiv.org/pdf/1303.5778.pdf][[pdf]]]
** *Deep neural networks for acoustic modeling in speech recognition:
  The shared views of four research groups* (2012), G. Hinton et al.
  [[http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf][[pdf]]]
** *Context-dependent pre-trained deep neural networks for
  large-vocabulary speech recognition* (2012) G. Dahl et al.
  [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&rep=rep1&type=pdf][[pdf]]]
** *Acoustic modeling using deep belief networks* (2012), A. Mohamed et
   al.
   [[http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf][[pdf]]]

#+BEGIN_HTML
  <!---[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Dong Yu](https://scholar.google.ca/citations?hl=en&user=tMY31_gAAAAJ)-->
#+END_HTML
** [2017] CTC (Connectionist Temporal Classification Loss) Explained
    :PROPERTIES:
    :AUTHOR:  Karl N.
    :YEAR:    2017
    :URL:     https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0
    :END:
*** Keypoints
**** In normal systems, we cut the audio signal into very small slices and feed them to RNN.
**** The predictions then become something like (for "CAT") -- "...C..A..AA..A..AA.T..TT.."
**** so obviously we need to get rid of the silence and repeats, the way to do that is CTC.
**** Essentially, the equation defines the loss that makes good probability distribution over good paths
* Reinforcement Learning / Robotics
** *End-to-end training of deep visuomotor policies* (2016), S. Levine
  et al.
  [[http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf][[pdf]]]
** *Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection* (2016), S. Levine et al.
  [[https://arxiv.org/pdf/1603.02199][[pdf]]]
** *Asynchronous methods for deep reinforcement learning* (2016), V. Mnih et al.
  [[http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf][[pdf]]]
** *Deep Reinforcement Learning with Double Q-Learning* (2016), H.
  Hasselt et al. [[https://arxiv.org/pdf/1509.06461.pdf][[pdf]]]
** *Mastering the game of Go with deep neural networks and tree search*
  (2016), D. Silver et al.
  [[http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html][[pdf]]]
** *Continuous control with deep reinforcement learning* (2015), T.
  Lillicrap et al. [[https://arxiv.org/pdf/1509.02971][[pdf]]]
** *Human-level control through deep reinforcement learning* (2015), V. Mnih et al.
  [[http://www.davidqiu.com:8888/research/nature14236.pdf][[pdf]]]
** *Deep learning for detecting robotic grasps* (2015), I. Lenz et al.
  [[http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf][[pdf]]]
** [2012] A painless Q-learning tutorial 
   :PROPERTIES:
   :AUTHOR:   John McCullock
   :YEAR:     2012
   :URL:      http://mnemstudio.org/path-finding-q-learning-tutorial.htm
   :END:
    :LOGBOOK:
    CLOCK: [2017-09-25 月 15:28]--[2017-09-25 月 15:53] =>  0:25
    :END:
*** keypoints
***** Q-learning is a reinforcement learning algorithm. It is suitable for problem which has finite number of states and we know the value of all state's immediate reward.
***** the main idea is do semi-random exploring to eventually map out an expected rewards value of that state. The expected value is the sum of current and all future rewards value (given discount factors).
***** So we will have a big rewards matrix (R) where row equals current state and column equals an action to next state. The values are the rewards when taking that action (and arriving at a new state).
***** We will also have a memory matrix (Q). which contains a sum of expected immediate and future rewards. Row is current state and column is the next future state.
***** the update formula is as follows:
****** Q(state,action) = R(current_state,action) + Gamma * max[ Q(immediate_next_state,all_actions) ]
******* where...
******* R = reward matrix
******* Q = memory matrix
******* Gamma = discount factor
******* This assumes a learning rate of 1. If we want a different learning rate, we can do:
******** Q_new = Q_old + learning_rate * (Q_update - Q_old)
** [2013] Playing atari with deep reinforcement learning  
   :PROPERTIES:
   :AUTHOR:   V. Mnih et al.
   :YEAR:     2013
   :URL:      http://arxiv.org/pdf/1312.5602.pdf
   :END:
*** keypoints
**** aasdf
** [2017] A Brief Survey of Deep Reinforcement Learning
   :PROPERTIES:
   :AUTHOR:   Kai Arulkumaran
   :YEAR:     2016
   :URL:      https://arxiv.org/pdf/1708.05866
   :END:
*** keypoints
**** In this survey, we begin withan introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms indeep  reinforcement  learning,  including  the  deep Q-network,trust region policy optimisation, and asynchronous advantage actor-critic.
**** General RL concepts
***** Reward-Driver Behavior
****** the essense of RL is interaction. the interaction loop is simple.
******* 1. given current state --> choose action
******* 2. execute action
******* 3. arrives at new state (received new state data and its rewards)
******* 4. go to 1. until terminal state
****** Per sequence above, we want to derive "optimal policy" so that the agents can asymtotically get "optimal" rewards --> which means a highest expected value of aggregated future rewards with a certain discount factor.
****** Formally, RL can be described as a Markov decision process (MDP). For (only) partially-observable states like in the real world, there is a generalization of MDP called POMDP.
****** Challenges in RL: long sequences until reward (credit assignment problem) and temporal sequence correlation
***** Reinforcement Learning Algorithms
****** Concept I: estimating Value function (total expected Rewards)
******* Dynamic Programming: 
******** define: V = total expected Rewards (R) , Q|s,a is conditional V given state s and action a
******** define: Y = R(t) + disc * Q|s(t+1),a(t+1)
******** define: Temporal difference (TD) error = Y - Q|s,a 
******** to get Q|s,a , we use Q-learning method and try to minimize the TD error
****** Concept II: sampling -- random walk till the end to get all Rs
******* so instead of going breadth-search like [I], we do depth-first
******* we can use Monte Carlo (MC) to get multiple returns and average them.
******* it is easier to learn that one actions lead to much better consequences than the other (a fork in the road)
******* define: relative advantage A = V - Q
*******  we use an idea of "advantage update" in many recent algorithms
****** Concept III: policy search
******* instead of estimating value function, we try to contruct policy directly. (so we can sample actions from it)
******* try several policies to get the optimal one, using either gradient-based or gradient-free optimization.
******* Policy Gradients
******** get the approximate V diff from different policies
******** interate policy parameters to know the diff on each one
******** change the params to optimize policy
******** there are several ways to estimate the diff -- Finite Diference, Likelihood Ratio etc.
******* Actor-Critic Methods
******** Use Actor (policy driven) to choose actions and learn feedback from Critic (value function).
******** Alphago uses this
****** Summary
******* Shallow sequence, no branching --> one-step TD learning
******* Shallow sequence, many branching --> dynamic programming
******* Deep sequences, no branching --> many-steps (MC) TD learning
******* Deep sequence, many branching --> exhaustive search
* GANs
* Style Transfers
Newly published papers (< 6 months) which are worth reading
** Deep Photo Style Transfer (2017), F. Luan et al.
[[http://arxiv.org/pdf/1703.07511v1.pdf][[pdf]]]
** Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. 
[[http://arxiv.org/pdf/1703.03864v1.pdf][[pdf]]]
** Deformable Convolutional Networks (2017), J. Dai et al.
[[http://arxiv.org/pdf/1703.06211v2.pdf][[pdf]]]
** Mask R-CNN (2017), K. He et al. 
[[https://128.84.21.199/pdf/1703.06870][[pdf]]]
** Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. 
[[http://arxiv.org/pdf/1703.05192v1.pdf][[pdf]]]
** Deep voice: Real-time neural text-to-speech (2017), S. Arik et al.,
[[http://arxiv.org/pdf/1702.07825v2.pdf][[pdf]]]
** [2017] PixelNet: Representation of the pixels, by the pixels, and for the pixels  
   :PROPERTIES:
   :AUTHOR:   A. Bansal et al.
   :YEAR:     2017
   :URL:      http://arxiv.org/pdf/1702.06506v1.pdf
   :END:
*** keypoints
**** This paper build on many recent ideas and introduces one big idea of its own (for segmentation)
**** recent ideas is using "hypercolumn" map as an input the FC layer.
***** "hypercolumn" means every feature map at every layer for a particular input pixel
**** new idea is the "sampling only some pixels" for output instead of doing the whole image output prediction
***** this is called "sparse prediction" vs "dense or full prediction"
***** the premise is that as nearby pixels are highly correlated, just sampling is sufficient for learning.
** Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe.
[[https://arxiv.org/abs/1702.03275][[pdf]]]
** Wasserstein GAN (2017), M. Arjovsky et al. 
[[https://arxiv.org/pdf/1701.07875v1][[pdf]]]
** Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. 
[[https://arxiv.org/pdf/1611.03530][[pdf]]]
** Least squares generative adversarial networks (2016), X. Mao et al.
[[https://arxiv.org/abs/1611.04076v2][[pdf]]]

* Credit card fraud detection
** [2014] Literature Survey
    :PROPERTIES:
    :AUTHOR:  Zeiler et al.
    :YEAR:     2014
    :URL:      http://www.ijmer.com/papers/Vol4_Issue9/Version-4/E0409_04-2431.pdf
    :END:
*** keypoints
***** algorithms
****** HMM
****** NN
****** Decision Tree
****** SVM
****** Genetic Algorithm
****** Meta Learning Strategy
****** Biologicla Immune System
* Weather Classification
** Overall Summary as of [2018-10]
There are no agreed upon public dataset and very few DL papers dedicated to the topic. 

The common dataset used is [2014] sunny/cloudy dataset with 10k images. Other recent papers [2018] have contructed their own dataset which are not opened to public yet. However, BDD100K dataset also has weather attribute labeled, so we should be considering using that.

There are 3 type of models proposed thus far.
- [2014] traditional feature engineering then use SVM/other clustering methods.
- [2015] pure CNN feature extraction then classify
- [2018] CNN-RNN and/or the combination of DL and traditional features.

so far the DL method did aggressively out-perform traditional ones.

New alternative would be to add new sensor data (temperature/humidity) and ensemble with CNN model. For that matter, how accurate would predictions from sensor data alone be?
** [2018] (2 Dataset) A CNN–RNN architecture for multi-label weather recognition
    :PROPERTIES:
    :AUTHOR:   Zhao et al.
    :YEAR:     2018
    :URL:      use sci-hub
    :END:
*** keypoints
**** recognize that weather classes are not exclusive to each other (for example, can be both sunny and foggy) so should classify accordingly (not using softmax or binary)
**** add 2 new datasets (8k - 7 classes) and (10k - 5 classes) for multi-labeling comparison
**** use CNNs as feature extractor
**** use "channel-wise attentions" which is a set of weights to amplify/lower each channel' response.
**** use "Convolutional" LSTM to retain spatial information (not flattening to 1-D vectors) 
**** flatten the output "hidden state" to predict weather class
**** then we repeat the step (in LSTM + getting new attention weights) to predict next weather class. If there are 5 classes, the LSTM will run for 5 steps. (This is weird.. because the problem is not time-based. and this runs from single image input)
** [2018] (Dataset)(Bad) Weather Classification: A new multi-class dataset, data augmentation approach and comprehensive evaluations of CNNs
    :PROPERTIES:
    :AUTHOR:   Guerra et al.
    :YEAR:     2018
    :URL:      https://arxiv.org/abs/1808.00588v1
    :END:
*** keypoints
**** new dataset (3K) - use 3 classes (rain, fog, snow) with equal split
**** later add sunny/cloudy from past dataset to get 5k (again, equal split)
**** In addition to raw image, they use superpixel (algo to cluster pixels together for further processing - google it) to ovelay on the image then feed to CNN feature extractors
**** finally, use some sort of SVMs as binary classifier for each class
**** overall achieved around 80-90% accuracy, with Resnet50 being the best extractor overall.
**** however, no mention of baseline (w/o superpixel) comparison. No justification of doing things, even just running their model through old sunny/cloudy dataset for comparison. bad paper.
** [2017] (Dataset) (Bad) Transfer Learning for Rain Detection in Images
    :PROPERTIES:
    :AUTHOR:   Alecci et al.
    :YEAR:     2017
    :URL:      https://repository.tudelft.nl/islandora/object/uuid%3A3bf546c0-a254-4c72-9ee4-02a0919c1624
    :END:
*** keypoints
**** tried Resnet-18 with various experiments on custom 400k rain-no-rain dataset
**** just bad all around. specific optimization to specific dataset. no baseline model. not useful.
** [2015] Weather Classification with Deep Convolutional Network
    :PROPERTIES:
    :AUTHOR:   Elhoseiny et al.
    :YEAR:     2015
    :URL:    http://www.academia.edu/18539252/WEATHER_CLASSIFICATION_WITH_DEEP_CONVOLUTIONAL_NEURAL_NETWORKs
    :END:
*** keypoints
**** use sunny/cloudy 10k dataset
**** applies AlexNet architecture to this problem
**** also compared the pretrained with ImageNet AlexNet + SVM vs train with weather data from scratch - conclusion is earlier base layers are quite general
**** achieved 91% accuracy (82% normalized)
** [2014] (Dataset) Two-class Weather Classification (with sunny/cloudy 10k dataset)
    :PROPERTIES:
    :AUTHOR:  Lu et al.
    :YEAR:     2014
    :URL:      http://www.cse.cuhk.edu.hk/leojia/projects/weatherclassify/index.htm
    :END:
*** keypoints
**** introduces the 10k weather dataset with 2 classes - sunny and cloudy
**** use traditional computer vision method to classify
***** custom feature engineering extracting 5 features -- sky, shadow, reflection, contrast, haze.
***** concat all features into 621-D vectors then use complex voting schemes to classify based on the existing of combinations of features. Tried SVM but didn't work well.
***** achieved 76% accuracy (53% normalized)
* Autonomous car driving
** overview paper
*** [2017-02] https://www.mdpi.com/2075-1702/5/1/6
* Face Detection
** Dataset: WiderFace
*** http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/
*** 30K images, 400k faces.
*** metric is PR curve, split by easy / medium / hard cases
** [2004] Robust Real-time Object Detection (Viola-Jones) 
*** [link to paper](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-IJCV-01.pdf)
*** Traditional system with impressive performance
    Input = 384x288 grayscale image, 15 FPS on 700 Mhz Intel Pentium III
*** Algo = Simple Features + Adaboost + Cascade
    1. Features = sum of two regions and diffs with each other (for every pixel coordinate)
    2. Since there are a lot of features, use Adaboost select a set of strongest weak classifiers
        weak classifer is basically this --> H = if single_feature > threshold then 1 else 0
    3. Attentional cascade - train a simple 2-feature classifier to simply reject no-face image. 
        Then queue up all the sub-windows (overlap cropping?), evaluate and reject, 
        then use stronger classifier from #2 on the remaining sub-windows.
** [2014] One millisecond face alignment with an ensemble of regression trees - Dlib uses this 
*** [link to paper](https://pdfs.semanticscholar.org/d78b/6a5b0dcaa81b1faea5fb0000045a62513567.pdf)
*** Use cascade of regressor method to detect facial landmarks (given that the image is already cropped to face area)
    claims 1 ms performance with unknown CPU. has error rate of 0.049 on HELEN face dataset. (2,000 training / 500 test image)
*** Algo = Default positions + features + gradient boosting + cascade
    * we can set up a default landmark (smiley face) in the image center or do an average of positions from a big dataset.  
    * then we regress -- computing an update regressors for each landmark x,y --> moving them closer to the face in image.
    * the features for regressions are diff in pixel intensities, the pixel coordinate is relative to the default face shape.  

  #### [2017] FaceBoxes: A CPU Real-time Face Detector with High Accuracy   ( [link to paper](https://arxiv.org/abs/1708.05234) )
*** custom (light-weight) CNN architecture. No novel idea. (the paper has a good summary of past papers however)
    * runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA (640x480) images.
*** some strategy for lightweighted architecture
    * reduce spatial size of input as quickly as possible
    * choose suitable kernel size - in their case it's 7x7, 5x5, 3x3
    * reduce number of output channel
    * use multi-scale anchor boxes output, but know where to have "dense" number of predictions.
*** postprocessing is common pipeline: lots of prediction > thresholding prob > NMS.

** [2017] Deep Face Recognition: A Survey 
*** [link to paper](https://arxiv.org/abs/1804.06655v1) )
*** Good review of modern face recognition systems. collections of recent techniques. It`s not face detection though.
** [2018] SFace: An Efficient Network for Face Detection in Large Scale Variations (Megvii Inc. Face++)
*** [link to paper](https://arxiv.org/abs/1804.06559v2)
*** A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations.  
**** The SFace architecture shows promising results on the new 4K-Face benchmarks. 
**** In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.
** Benchmark - Labeled Faces in the Wild (LFW) dataset - [state of the art results](http://vis-www.cs.umass.edu/lfw/results.html#UnrestrictedLb)
*** most commercial systems get > 99.0% classification accuracy, including Dlib
*** update as of beginning of 2018

* Own discovery of Research Papers
** Mobilenets
*** [[https://arxiv.org/pdf/1704.04861.pdf#page=1&zoom=140,-205,792]]
*** from google
** [2011] Adaptive Deconvolutional Networks for Mid and High Level Feature Learning
    :PROPERTIES:
    :AUTHOR:  Zeiler et al.
    :YEAR:     2011
    :URL:      http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf
    :END:
*** keypoints
***** iterations from the 2010 paper, add unpooling reconstrucitons with switches (location info for the max-pool values)
***** they are able to re-create the input-size map for all layers
** [2010] Deconvolutional Networks
    :PROPERTIES:
    :AUTHOR:  Zeiler et al.
    :YEAR:     2010
    :URL:      http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf
    :END:
*** keypoints
***** Deconvolution is actually "transposed convolution"
***** essentially, it uses feature map to compose back to the original images, like legos.
***** The kernels are different from the feed-forward kernels, of course.
***** the usage of "sparse coding" made this possible. see: [[http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic][tranposed convolution arithmetic]]
***** see answer from here: [[https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers][stackexchange]]
***** good slide here: http://cs.nyu.edu/~fergus/drafts/utexas2.pdf
** [2016] Learning Deep Features for Discriminative Localization (global average pooling)
    :PROPERTIES:
    :AUTHOR:  Bolei Zhou
    :YEAR:     2016
    :URL:      http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf
    :END:
*** keypoints
***** using "global average pooling" method with each featuremap on the last layer of CNN.
***** then we can use the FC weights to combined the GAP values.
***** this effectively "focuses" the network activations before connecting to FC layer.
***** with this we can generate heatmap to see the activation overlays
** [2015] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
    :PROPERTIES:
    :AUTHOR:  Vijay Badrinarayanan
    :YEAR:     2015
    :URL:      https://arxiv.org/pdf/1511.00561.pdf
    :END:
*** keypoints
***** this is basically an autodecoder, except for CNN architecture. Also use final targets as the segmentation labels.
** [2011] How Brains Are Built: Principles of Computational Neuroscience
    :PROPERTIES:
    :AUTHOR:  Richard Granger
    :YEAR:     2011
    :URL:      https://arxiv.org/pdf/1704.03855.pdf
    :END:
*** keypoints
***** precise simulation of the brain chemically is very difficult. However, we can possibly create the brain model that is "computationally" accurate. we can even use this model to experiment and fix what's wrong with our brain.
***** Computationally means to understand the subject functions -- enough to create a replica of them. For example, we don't yet understand everything about kidneys about we can create artificial ones that works well now.
***** What we know now: very little, but we know some "constraint" rules
****** brain component allometry -- relative size of the brain components vs overall size. The relationship holds across all animal size.
****** telencephalic uniformity -- neurons throughout the forebrain has similar, repeatable designs with only few exceptions. This means there is a general representation of a wide variety of tasks -- audio, visual , touch etc.
****** anatomical and physiological imprecision -- the neurons are slow and sloppy (probabilistic). However, the brain is overall working in a robust way.. how?
****** task specification -- a classification given freeform input. One example is a call support desk. Given a free-form input, direct the customer to appropriate channels. It is highly contextual and no hard rules applied.
****** parallel processing -- the neuron circuits are painfully slow compared to computer CPU, it seems that the power of the brain lies in its massively parrallel computing power.
***** Current progress
****** basal ganglia -- this is the area that receive sensory input, manage reward and punishments mechanism, and learn motor skills. We are close to computationally simulate this.
****** neocortex -- yeah, no way we are close. Interestingly, the neocortex is connected with basal ganglia through a loop. We are close to successfully creating all the sensory prosthetics, but no way close to simulating the neocortex (higher thoughts).
****** the most exciting area of research today is about how the neocortex encode the internal representations of concepts and objects.
* Other papers still unassorted
** [2017] A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks
    :PROPERTIES:
    :AUTHOR:  Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher
    :YEAR:     2017
    :URL:      https://openreview.net/forum?id=SJZAb5cel
    :END:
*** keypoints
**** ABSTRACT: 
***** Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. 
***** Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. 
***** We use a simple regularization term to allow for optimizing all model weights to improve one task’s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. 
***** It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.
**** This is kind of like Ensembling models, but they are more "joined" at the end (softmax layer and feature layer), rather than just averaging results from softmax.
** [2017] Hierarchical Memory Networks
    :PROPERTIES:
    :AUTHOR:  Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio
    :YEAR:     2017
    :URL:      https://arxiv.org/pdf/1704.03855.pdf
    :END:
*** keypoints
**** ABSTRACT:
***** Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. 
***** The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. 
***** However, this is not computationally scalable for applications which require the network to read from extremely large memories.  
***** On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully.  
***** In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks.  
***** The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory.  
***** Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network.  
***** We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.
* Articles and Videos
** [2017] The End of Human Doctors (series)
    :PROPERTIES:
    :AUTHOR:  Luke Rayner
    :YEAR:     2017
    :URL:      https://lukeoakdenrayner.wordpress.com/2017/04/20/the-end-of-human-doctors-introduction/
    :END:
*** Part 2: Understanding Medicine
**** Most of the tasks Medical doctors do are related to "perception", not "decision making". The later part is relatively fast and has been done better by the Machine since MYCIN.
**** perceptual tasks like identifying tree-shape patterns in X-rays -- Deep learning is very good at it.
**** Most susceptible specialties are Radiology and Pathology, comprising of 25% of doctors (in Australia).
*** Part 3: Understanding Automation
**** Automation replaces tasks, not jobs. How much time the task takes a human determines how many jobs are lost.
**** Machines that “help” or “augment” humans still destroy jobs and lower wages.
**** Hybrid-chess does not prove that human/machine teams are better than computers alone. STOP SAYING THIS, tech people!
**** Deep learning threatens tasks that make up a terrifyingly large portion of doctors’ jobs.
**** In the developed world, demand for medical services may be unable to increase as prices fall due to automation, which normally protects jobs.
*** Part 4: Radiology Escape Velocity
**** even if the rate of automation of 5% per year, in 30 years there will still be one-third the current radiologist workforce remaining.
*** Part 5: Understanding Regulation
**** In case of USA, it usually takes 3 to 10 years to go through the whole process from concept to approval to use in the medical industry.
**** "measurements"-related technology can opt to go through case-I (low-risk type) route with substantially shorter time to approval.
**** There are two approach in using computer technology
***** measurements to aid doctors' decisions. (CADe) --  doctors disliked them, not doing well as a result.
***** measurements AND diagnosis (CADx) -- never been approved by FDA before.
**** Conclusion: current regulation in developed countries is SUPER conservative and so it will take a lot of time and money to get new technology adopted. Not so for developing world, we might see it much faster there.
*** Part 6: Current State-of-the-Art results and impact
**** Stanford (and collaborators) trained a system to identify skin lesions that need a biopsy. Skin cancer is the most common malignancy in light-skinned populations.
**** This is a useful clinical task, and is a large part of current dermatological practice.
**** They used 130,000 skin lesion photographs for training, and enriched their training and test sets with more positive cases than would be typical clinically.
**** The images were downsampled heavily, discarding  around 90% of the pixels.
**** They used a “tree ontology” to organise the training data, allowing them to improve their accuracy by training to recognise 757 classes of disease. This even improved their results on higher level tasks, like “does this lesion need a biopsy?”
**** They were better than individual dermatologists at identifying lesions that needed biopsy, with more true positives and less false positives.
**** While there are possible regulatory issues, the team appears to have a working smartphone application already. I would expect something like this to be available to consumers in the next year or two.
**** The impact on dermatology is unclear. We could actually see shortages of dermatologists as demand for biopsy services increases, at least in the short term.
** [2017] (Video) Geometric Deep Learning | || Radcliffe Institute
    :PROPERTIES:
    :AUTHOR:  Michael Bronstein
    :YEAR:     2017
    :URL:      https://www.youtube.com/watch?v=ptcBmEHDWds
    :END:
*** keypoints
**** Identical twins (Alex & Michael) -- study and worked in the same field (Computer Vision)
**** Invented what became the Kinect camera sensor
**** Keys for recognizing face:
***** Humans actually recognize people based on "texture" appearance, not the 3D geometry
***** facial expressions changed the projected texture to 2D, but not the actual texture if projected on the plane
***** Therefore, we can use the "geodesic" distance instead of euclidean distance to measure the actual distance between important face features. If the distances are approximately the same, then it's the same face.
***** Thee kind of techniques have been use to recognize diferent faces, including identical twins.
***** Geometric deep learning: applying CNNs on 3D surface via heat diffusion equation.
****** Use Case: Recognition, social network analysis, recommender systems
** [2015] Visual explanation of Information Theory
    :PROPERTIES:
    :AUTHOR:  Colah
    :YEAR:    2015
    :URL:     http://colah.github.io/posts/2015-09-Visual-Information/ 
    :END:
*** keypoints
**** Shannon's Entropy formula - H(X)
***** this is a way to estimate how many bits are needed to encode given information with certain distributions
***** the estimated bits are from the best possible encodings ("optimized")
***** H(X) = P(X)*log2(1/P(X)) where P(X) means probabilty of X
**** some interesting permutation give conditional probabilities
***** P(X,Y) = P(X)*P(Y|X) = P(Y)*P(X|Y)
***** H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
***** H(X|Y) = sum{P(X,Y)*log2(1/P(X|Y))}
**** then we can derive "mutual" [I] and "variational" [V] information
***** I(X,Y) = H(X,Y) - H(X) - H(Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
***** V(X,Y) = H(X,Y) - I(X,Y)
**** KL-divergence [D] or [K]
***** Dy(x) = K(X||Y) = H(X,Y) - H(X)
***** This is a way to see how the new distribution (Y) is close to the original distribution (X)
***** if it is the same, then KL is zero, otherwise it has value. 
***** this is not a symmetric measure. K(X||Y) <> K(Y||X)
* Snippets
** Backbone feature extractor short summary
*** source: https://arxiv.org/pdf/1804.06215.pdf
*** summary
The backbone network for object detection are usually borrowed from the ImageNet classification.  

Many new networks are designed to get higher performance for ImageNet. AlexNet (2012) is among the first to try to increase the depth of CNN. In order to reduce the network computation and increase the valid receptive field, AlexNet down-samples the feature map with 32 strides which is a standard setting for the following works. It also implemented group convolutions (branch into two CNN tracks to train on seperate GPU simutaneously) but mostly because of engineering constraint (3GB VRAM limit)

VGGNet (2014) stacks 3x3 convolution operation to build a deeper network, while still involves 32 strides in feature maps. Most of the following researches adopt VGG like structure, and design a better component in each stage (split by stride).

GoogleNet (2015) proposes a novel inception block to involve more diversity features.

ResNet (2015) adopts “bottleneck” design with residual sum operation in each stage, which has been proved a simple and efficient way to build a deeper neural network.

ResNext (2016) and Xception (2016) use group convolution layer to replace the traditional convolution. It reduces the parameters and increases the accuracy simultaneously.

DenseNet densely concat several layers, it further reduces parameters while keeping competitive accuracy. Another different research is Dilated Residual Network which extracts features with less strides. DRN achieves notable results on segmentation, while has little discussion on object  detection. There are still lots of research for efficient backbone, such as [17,15,16]. However they are usually designed for classification.

** asdf
* Classic Papers
/Classic papers published before 2012/ 
** An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al.
[[http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf][[pdf]]]
** Deep sparse rectifier neural networks (2011), X. Glorot et al.
[[http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf][[pdf]]]
** Natural language processing (almost) from scratch (2011), R. Collobert
et al. [[http://arxiv.org/pdf/1103.0398][[pdf]]]
** Recurrent neural network based language model (2010), T. Mikolov et al.
[[http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf][[pdf]]]
** Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al.
[[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&rep=rep1&type=pdf][[pdf]]]
** Learning mid-level features for recognition (2010), Y. Boureau
[[http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf][[pdf]]]
** A practical guide to training restricted boltzmann machines (2010), G. Hinton
[[http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf][[pdf]]]
** Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio
[[http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf][[pdf]]]
** Why does unsupervised pre-training help deep learning (2010), D. Erhan et al.
[[http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf][[pdf]]]
** Learning deep architectures for AI (2009), Y. Bengio.
[[http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf][[pdf]]]
** Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al.
[[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&rep=rep1&type=pdf][[pdf]]]
** Greedy layer-wise training of deep networks (2007), Y. Bengio et al.
[[http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf][[pdf]]]
** Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov.
[[http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf][[pdf]]]
** A fast learning algorithm for deep belief nets (2006), G. Hinton et al.
[[http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf][[pdf]]]
** Gradient-based learning applied to document recognition (1998), Y. LeCun et al.
[[http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf][[pdf]]]
** Long short-term memory (1997), S. Hochreiter and J. Schmidhuber.
[[http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735][[pdf]]]

* HW / SW / Dataset
** OpenAI gym (2016), G. Brockman et al.
  [[https://arxiv.org/pdf/1606.01540][[pdf]]]
** TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al.
  [[http://arxiv.org/pdf/1603.04467][[pdf]]]
** Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.
** Torch7: A matlab-like environment for machine learning, R. Collobert et al.
  [[https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf][[pdf]]]
** MatConvNet: Convolutional neural networks for matlab (2015), A.
  Vedaldi and K. Lenc [[http://arxiv.org/pdf/1412.4564][[pdf]]]
** Imagenet large scale visual recognition challenge (2015), O.
  Russakovsky et al. [[http://arxiv.org/pdf/1409.0575][[pdf]]]
** Caffe: Convolutional architecture for fast feature embedding (2014),
  Y. Jia et al. [[http://arxiv.org/pdf/1408.5093][[pdf]]]

* Book / Survey / Review
** On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj.
  [[https://arxiv.org/pdf/1702.07800][[pdf]]]
** Deep Reinforcement Learning: An Overview (2017), Y. Li,
  [[http://arxiv.org/pdf/1701.07274v2.pdf][[pdf]]]
** Neural Machine Translation and Sequence-to-sequence Models(2017): A
  Tutorial, G. Neubig. [[http://arxiv.org/pdf/1703.01619v1.pdf][[pdf]]]
** Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen.
  [[http://neuralnetworksanddeeplearning.com/index.html][[html]]]
** Deep learning (Book, 2016), Goodfellow et al.
  [[http://www.deeplearningbook.org/][[html]]]
** LSTM: A search space odyssey (2016), K. Greff et al.
  [[https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer][[pdf]]]
** Tutorial on Variational Autoencoders (2016), C. Doersch.
  [[https://arxiv.org/pdf/1606.05908][[pdf]]]
** Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton
  [[https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf][[pdf]]]
** Deep learning in neural networks: An overview (2015), J. Schmidhuber
  [[http://arxiv.org/pdf/1404.7828][[pdf]]]
** Representation learning: A review and new perspectives (2013), Y.
  Bengio et al. [[http://arxiv.org/pdf/1206.5538][[pdf]]]

* Video Lectures / Tutorials / Blogs
** (Lectures) 
*** CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University 
[[http://cs231n.stanford.edu/][[web]]]
*** CS224d, Deep Learning for Natural Language Processing, Stanford University  
[[http://cs224d.stanford.edu/][[web]]]
*** Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford 
[[https://github.com/oxford-cs-deepnlp-2017/lectures][[web]]]

** (Tutorials) 
*** NIPS 2016 Tutorials, Long Beach
[[https://nips.cc/Conferences/2016/Schedule?type=Tutorial][[web]]]
*** ICML 2016 Tutorials, New York City
[[http://techtalks.tv/icml/2016/tutorials/][[web]]]
*** ICLR 2016 Videos, San Juan 
[[http://videolectures.net/iclr2016_san_juan/][[web]]]
*** Deep Learning Summer School 2016, Montreal
[[http://videolectures.net/deeplearning2016_montreal/][[web]]]
*** Bay Area Deep Learning School 2016, Stanford
[[https://www.bayareadlschool.org/][[web]]]

** (Blogs)
*** OpenAI  
[[https://www.openai.com/][[web]]]
*** Distill
[[http://distill.pub/][[web]]]
*** Andrej Karpathy Blog
[[http://karpathy.github.io/][[web]]]
*** Colah's Blog
[[http://colah.github.io/][[Web]]]
*** WildML
[[http://www.wildml.com/][[Web]]]
*** FastML
[[http://www.fastml.com/][[web]]]
*** TheMorningPaper
[[https://blog.acolyer.org][[web]]]

